{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HjJHYcWNcqu"
      },
      "source": [
        "### SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjezA2lKgIAe",
        "outputId": "e41c6c17-2bdd-405d-9f7b-50a496c85140"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastavro\n",
            "  Downloading fastavro-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Installing collected packages: fastavro\n",
            "Successfully installed fastavro-1.9.4\n",
            "Collecting faker\n",
            "  Downloading Faker-24.9.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "Installing collected packages: faker\n",
            "Successfully installed faker-24.9.0\n",
            "Collecting timezonefinder\n",
            "  Downloading timezonefinder-6.5.0.tar.gz (49.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi<2,>=1.15.1 in /usr/local/lib/python3.10/dist-packages (from timezonefinder) (1.16.0)\n",
            "Collecting h3<4,>=3.7.6 (from timezonefinder)\n",
            "  Downloading h3-3.7.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1.23 in /usr/local/lib/python3.10/dist-packages (from timezonefinder) (1.25.2)\n",
            "Requirement already satisfied: setuptools>=65.5 in /usr/local/lib/python3.10/dist-packages (from timezonefinder) (67.7.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi<2,>=1.15.1->timezonefinder) (2.22)\n",
            "Building wheels for collected packages: timezonefinder\n",
            "  Building wheel for timezonefinder (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for timezonefinder: filename=timezonefinder-6.5.0-cp310-cp310-manylinux_2_35_x86_64.whl size=49412386 sha256=35924f39afd18ed00ffbd1f261266e61e72841294b93ec08686e6eec02aa3559\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/ae/2d/8bb82cff928de1ce64feef734cf9ef78d84f026693fecc5617\n",
            "Successfully built timezonefinder\n",
            "Installing collected packages: h3, timezonefinder\n",
            "Successfully installed h3-3.7.7 timezonefinder-6.5.0\n",
            "Requirement already satisfied: fastavro in /usr/local/lib/python3.10/dist-packages (1.9.4)\n",
            "Collecting kafka-python\n",
            "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.5/246.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kafka-python\n",
            "Successfully installed kafka-python-2.0.2\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install fastavro pandas\n",
        "%pip install faker\n",
        "%pip install timezonefinder\n",
        "%pip install fastavro kafka-python\n",
        "%pip install -q pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulJcuk4CgIAh",
        "outputId": "d59551ab-dfd6-4fec-c22f-4100ecddfae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing environment.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile environment.sh\n",
        "#!/usr/bin/bash\n",
        "export KAFKA_BINARY_VERSION='3.7.0'\n",
        "export SCALA_BINARY_VERSION='2.13'\n",
        "export KAFKA_BINARY_VERSION=$KAFKA_BINARY_VERSION\n",
        "export SCALA_BINARY_VERSION=$SCALA_BINARY_VERSION\n",
        "export PATH=$PATH:$PWD/kafka_$SCALA_BINARY_VERSION-$KAFKA_BINARY_VERSION/bin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00O0ugghNGXu"
      },
      "source": [
        "## KAFKA SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ99-4aVgIAi",
        "outputId": "be0cc27f-6051-43d8-fb1b-3799f5eb2b49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing kafka_setup.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile kafka_setup.sh\n",
        "\n",
        "source ./environment.sh\n",
        "echo kafka_$SCALA_BINARY_VERSION-$KAFKA_BINARY_VERSION\n",
        "echo $PATH\n",
        "\n",
        "# Java Setup\n",
        "wget -O- https://apt.corretto.aws/corretto.key | sudo apt-key add -\n",
        "sudo add-apt-repository 'deb https://apt.corretto.aws stable main' -y\n",
        "sudo apt-get -y update; sudo apt-get install -y java-11-amazon-corretto-jdk\n",
        "\n",
        "# Kafka Setup\n",
        "wget https://downloads.apache.org/kafka/${KAFKA_BINARY_VERSION}/kafka_${SCALA_BINARY_VERSION}-${KAFKA_BINARY_VERSION}.tgz\n",
        "tar xzf kafka_${SCALA_BINARY_VERSION}-${KAFKA_BINARY_VERSION}.tgz\n",
        "\n",
        "UUID=$(./kafka_${SCALA_BINARY_VERSION}-${KAFKA_BINARY_VERSION}/bin/kafka-storage.sh random-uuid)\n",
        "echo \"export UUID=$UUID\" >> ./environment.sh\n",
        "cat environment.sh\n",
        "\n",
        "# Start Kafka Broker\n",
        "\n",
        "echo kafka_$SCALA_BINARY_VERSION-$KAFKA_BINARY_VERSION\n",
        "\n",
        "# offsets.retention.minutes determines how long Kafka retains the commit offsets for consumer groups.\n",
        "echo \"offsets.retention.minutes=300\" >> ./kafka_${SCALA_BINARY_VERSION}-${KAFKA_BINARY_VERSION}/config/kraft/server.properties\n",
        "\n",
        "./kafka_${SCALA_BINARY_VERSION}-${KAFKA_BINARY_VERSION}/bin/kafka-storage.sh format -t ${UUID} -c ./kafka_${SCALA_BINARY_VERSION}-${KAFKA_BINARY_VERSION}/config/kraft/server.properties\n",
        "nohup ./kafka_${SCALA_BINARY_VERSION}-${KAFKA_BINARY_VERSION}/bin/kafka-server-start.sh ./kafka_${SCALA_BINARY_VERSION}-${KAFKA_BINARY_VERSION}/config/kraft/server.properties > kafka_server.log &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UBEdCS2gIAi",
        "outputId": "508e7f94-d258-4cd9-d298-87e313153439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kafka_2.13-3.7.0\n",
            "/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/content/kafka_2.13-3.7.0/bin\n",
            "--2024-04-15 19:29:35--  https://apt.corretto.aws/corretto.key\n",
            "Resolving apt.corretto.aws (apt.corretto.aws)... 18.66.255.23, 18.66.255.118, 18.66.255.63, ...\n",
            "Connecting to apt.corretto.aws (apt.corretto.aws)|18.66.255.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1695 (1.7K) [binary/octet-stream]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]   1.66K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-15 19:29:35 (4.38 MB/s) - written to stdout [1695/1695]\n",
            "\n",
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "OK\n",
            "Repository: 'deb https://apt.corretto.aws stable main'\n",
            "Description:\n",
            "Archive for codename: stable components: main\n",
            "More info: https://apt.corretto.aws\n",
            "Adding repository.\n",
            "Adding deb entry to /etc/apt/sources.list.d/archive_uri-https_apt_corretto_aws-jammy.list\n",
            "Adding disabled deb-src entry to /etc/apt/sources.list.d/archive_uri-https_apt_corretto_aws-jammy.list\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:5 https://apt.corretto.aws stable InRelease [10.7 kB]\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,974 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,375 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,691 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,082 kB]\n",
            "Get:16 https://apt.corretto.aws stable/main amd64 Packages [17.1 kB]\n",
            "Get:17 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [808 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [27.8 kB]\n",
            "Fetched 7,238 kB in 5s (1,517 kB/s)\n",
            "Reading package lists... Done\n",
            "W: https://apt.corretto.aws/dists/stable/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:6 https://apt.corretto.aws stable InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: https://apt.corretto.aws/dists/stable/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  java-11-amazon-corretto-jdk\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 195 MB of archives.\n",
            "After this operation, 327 MB of additional disk space will be used.\n",
            "Get:1 https://apt.corretto.aws stable/main amd64 java-11-amazon-corretto-jdk amd64 1:11.0.22.7-1 [195 MB]\n",
            "Fetched 195 MB in 3s (62.5 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package java-11-amazon-corretto-jdk:amd64.\n",
            "(Reading database ... 121752 files and directories currently installed.)\n",
            "Preparing to unpack .../java-11-amazon-corretto-jdk_1%3a11.0.22.7-1_amd64.deb ...\n",
            "Unpacking java-11-amazon-corretto-jdk:amd64 (1:11.0.22.7-1) ...\n",
            "Setting up java-11-amazon-corretto-jdk:amd64 (1:11.0.22.7-1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/java to provide /usr/bin/java (java) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/keytool to provide /usr/bin/keytool (keytool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/rmid to provide /usr/bin/rmid (rmid) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/rmiregistry to provide /usr/bin/rmiregistry (rmiregistry) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jjs to provide /usr/bin/jjs (jjs) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/pack200 to provide /usr/bin/pack200 (pack200) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/unpack200 to provide /usr/bin/unpack200 (unpack200) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/javac to provide /usr/bin/javac (javac) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jaotc to provide /usr/bin/jaotc (jaotc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jlink to provide /usr/bin/jlink (jlink) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jmod to provide /usr/bin/jmod (jmod) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jhsdb to provide /usr/bin/jhsdb (jhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jar to provide /usr/bin/jar (jar) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jarsigner to provide /usr/bin/jarsigner (jarsigner) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/javadoc to provide /usr/bin/javadoc (javadoc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/javap to provide /usr/bin/javap (javap) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jcmd to provide /usr/bin/jcmd (jcmd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jdb to provide /usr/bin/jdb (jdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jdeps to provide /usr/bin/jdeps (jdeps) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jdeprscan to provide /usr/bin/jdeprscan (jdeprscan) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jimage to provide /usr/bin/jimage (jimage) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jinfo to provide /usr/bin/jinfo (jinfo) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jmap to provide /usr/bin/jmap (jmap) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jps to provide /usr/bin/jps (jps) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jrunscript to provide /usr/bin/jrunscript (jrunscript) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jshell to provide /usr/bin/jshell (jshell) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jstack to provide /usr/bin/jstack (jstack) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jstat to provide /usr/bin/jstat (jstat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jstatd to provide /usr/bin/jstatd (jstatd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/rmic to provide /usr/bin/rmic (rmic) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/serialver to provide /usr/bin/serialver (serialver) in auto mode\n",
            "--2024-04-15 19:30:06--  https://downloads.apache.org/kafka/3.7.0/kafka_2.13-3.7.0.tgz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 88.99.208.237, 135.181.214.104, 2a01:4f9:3a:2c57::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|88.99.208.237|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 119028138 (114M) [application/x-gzip]\n",
            "Saving to: ‘kafka_2.13-3.7.0.tgz’\n",
            "\n",
            "kafka_2.13-3.7.0.tg 100%[===================>] 113.51M  27.7MB/s    in 4.7s    \n",
            "\n",
            "2024-04-15 19:30:11 (24.0 MB/s) - ‘kafka_2.13-3.7.0.tgz’ saved [119028138/119028138]\n",
            "\n",
            "#!/usr/bin/bash\n",
            "export KAFKA_BINARY_VERSION='3.7.0'\n",
            "export SCALA_BINARY_VERSION='2.13'\n",
            "export KAFKA_BINARY_VERSION=$KAFKA_BINARY_VERSION\n",
            "export SCALA_BINARY_VERSION=$SCALA_BINARY_VERSION\n",
            "export PATH=$PATH:$PWD/kafka_$SCALA_BINARY_VERSION-$KAFKA_BINARY_VERSION/bin\n",
            "export UUID=R_auVSBVTxKd7EmYXMYiIQ\n",
            "kafka_2.13-3.7.0\n",
            "metaPropertiesEnsemble=MetaPropertiesEnsemble(metadataLogDir=Optional.empty, dirs={/tmp/kraft-combined-logs: EMPTY})\n",
            "Formatting /tmp/kraft-combined-logs with metadata.version 3.7-IV4.\n",
            "nohup: redirecting stderr to stdout\n",
            "\tzookeeper.ssl.protocol = TLSv1.2\n",
            "\tzookeeper.ssl.truststore.location = null\n",
            "\tzookeeper.ssl.truststore.password = null\n",
            "\tzookeeper.ssl.truststore.type = null\n",
            " (kafka.server.KafkaConfig)\n",
            "[2024-04-15 19:30:23,653] INFO [BrokerServer id=1] Waiting for the broker to be unfenced (kafka.server.BrokerServer)\n",
            "[2024-04-15 19:30:23,688] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)\n",
            "[2024-04-15 19:30:23,688] INFO [BrokerServer id=1] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)\n",
            "[2024-04-15 19:30:23,690] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)\n",
            "[2024-04-15 19:30:23,690] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)\n",
            "[2024-04-15 19:30:23,690] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)\n",
            "[2024-04-15 19:30:23,694] INFO [BrokerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\n",
            "[2024-04-15 19:30:23,694] INFO [BrokerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\n",
            "[2024-04-15 19:30:23,694] INFO [BrokerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\n",
            "[2024-04-15 19:30:23,694] INFO [BrokerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\n",
            "[2024-04-15 19:30:23,694] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer)\n",
            "[2024-04-15 19:30:23,695] INFO Kafka version: 3.7.0 (org.apache.kafka.common.utils.AppInfoParser)\n",
            "[2024-04-15 19:30:23,695] INFO Kafka commitId: 2ae524ed625438c5 (org.apache.kafka.common.utils.AppInfoParser)\n",
            "[2024-04-15 19:30:23,695] INFO Kafka startTimeMs: 1713209423694 (org.apache.kafka.common.utils.AppInfoParser)\n",
            "[2024-04-15 19:30:23,697] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "%%shell\n",
        "source kafka_setup.sh\n",
        "sleep 10\n",
        "tail -20 kafka_server.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEpamyzwgIAj",
        "outputId": "4c3af829-f1b3-4320-ddf9-24a583ea47de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created topic spotifyWrapped.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "source ./environment.sh\n",
        "\n",
        "kafka-topics.sh --bootstrap-server 127.0.0.1:9092 --topic spotifyWrapped --create --partitions 3 --replication-factor 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCvwxMiDgIAj",
        "outputId": "147592e6-2df8-4127-95ea-54743b55b96b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing check_kafka_consumers.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile check_kafka_consumers.sh\n",
        "#!/usr/bin/env bash\n",
        "source ./environment.sh\n",
        "\n",
        "echo \"Active Consumer Groups\"\n",
        "while true\n",
        "do\n",
        "date\n",
        "kafka-consumer-groups.sh --bootstrap-server 127.0.0.1:9092 --describe --all-groups\n",
        "sleep 1\n",
        "done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bawn_-ougIAj",
        "outputId": "e46814dd-5d9c-4c9a-f067-758ef972f7df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: redirecting stderr to stdout\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "%%shell\n",
        "chmod +x ./check_kafka_consumers.sh\n",
        "nohup ./check_kafka_consumers.sh > kafka_consumers.log &"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHCcnPFwNJOV"
      },
      "source": [
        "## AVRO PRODUCER FILE GENERATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vNA117EXJBA"
      },
      "source": [
        "This file contains the data classes that describes the 8 different personalities with preferences, also the distributions extracted from the datasets, and the data generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPCCOcTzgIAk",
        "outputId": "94e5b039-ee1e-4427-b693-15128df70223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing avro_producer.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile avro_producer.py\n",
        "#!/usr/bin/python\n",
        "\n",
        "import fastavro\n",
        "from fastavro.schema import load_schema\n",
        "from fastavro import parse_schema, writer\n",
        "import pandas as pd\n",
        "import uuid\n",
        "from google.colab import files\n",
        "from datetime import datetime, timedelta, timezone\n",
        "import random\n",
        "from faker import Faker\n",
        "from timezonefinder import TimezoneFinder\n",
        "import pytz\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "from kafka import KafkaProducer\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.avro.functions import from_avro\n",
        "from pyspark.sql.functions import col\n",
        "import sys\n",
        "from kafka import KafkaConsumer\n",
        "import io\n",
        "import os\n",
        "import time\n",
        "\n",
        "fake = Faker()\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "@dataclass\n",
        "class ListeningPersonality:\n",
        "    name: str\n",
        "    genre_distribution: dict[str, float]\n",
        "    active_hours: Tuple[int, int]\n",
        "    interactions: dict\n",
        "    inter_prob: float\n",
        "\n",
        "personalities = [\n",
        "    ListeningPersonality(\n",
        "        name=\"RapRockJazzLover\",\n",
        "        genre_distribution={\"hip-hop\": 0.6, \"rock\": 0.3, \"jazz\": 0.1},\n",
        "        active_hours=(17, 23),  # Active from 5 PM to 11 PM\n",
        "        interactions={\"skip_probability\": 0.1, \"like_probability\": 0.3, \"playlist_add_probability\": 0.2},\n",
        "        inter_prob=0.3\n",
        "    ),\n",
        "    ListeningPersonality(\n",
        "        name=\"IndieSoulFan\",\n",
        "        genre_distribution={\"indie\": 0.5, \"soul\": 0.5},\n",
        "        active_hours=(9, 17),  # Active from 9 AM to 5 PM\n",
        "        interactions={\"skip_probability\": 0.15, \"like_probability\": 0.25, \"playlist_add_probability\": 0.1},\n",
        "        inter_prob=0.7\n",
        "    ),\n",
        "    ListeningPersonality(\n",
        "        name=\"ElectronicExplorer\",\n",
        "        genre_distribution={\"electronic\": 0.7, \"house\": 0.2, \"techno\": 0.1},\n",
        "        active_hours=(22, 4),  # Active late night to early morning\n",
        "        interactions={\"skip_probability\": 0.05, \"like_probability\": 0.2, \"playlist_add_probability\": 0.25},\n",
        "        inter_prob=0.5\n",
        "    ),\n",
        "    ListeningPersonality(\n",
        "        name=\"PopPunkPerson\",\n",
        "        genre_distribution={\"pop\": 0.4, \"punk\": 0.6},\n",
        "        active_hours=(15, 22),\n",
        "        interactions={\"skip_probability\": 0.2, \"like_probability\": 0.5, \"playlist_add_probability\": 0.3},\n",
        "        inter_prob=0.8\n",
        "    ),\n",
        "    ListeningPersonality(\n",
        "        name=\"ClassicalConnoisseur\",\n",
        "        genre_distribution={\"classical\": 1.0},\n",
        "        active_hours=(8, 20),\n",
        "        interactions={\"skip_probability\": 0.05, \"like_probability\": 0.4, \"playlist_add_probability\": 0.15},\n",
        "        inter_prob=0.9\n",
        "    ),\n",
        "    ListeningPersonality(\n",
        "        name=\"JazzJunkie\",\n",
        "        genre_distribution={\"jazz\": 0.9, \"blues\": 0.1},\n",
        "        active_hours=(18, 24),\n",
        "        interactions={\"skip_probability\": 0.1, \"like_probability\": 0.35, \"playlist_add_probability\": 0.25},\n",
        "        inter_prob=0.3\n",
        "    ),\n",
        "    ListeningPersonality(\n",
        "        name=\"CountryCruiser\",\n",
        "        genre_distribution={\"country\": 0.8, \"folk\": 0.2},\n",
        "        active_hours=(10, 18),\n",
        "        interactions={\"skip_probability\": 0.12, \"like_probability\": 0.3, \"playlist_add_probability\": 0.18},\n",
        "        inter_prob=0.4\n",
        "\n",
        "    ),\n",
        "    ListeningPersonality(\n",
        "        name=\"ReggaeRelaxer\",\n",
        "        genre_distribution={\"reggae\": 0.7, \"dancehall\": 0.3},\n",
        "        active_hours=(16, 23),\n",
        "        interactions={\"skip_probability\": 0.08, \"like_probability\": 0.4, \"playlist_add_probability\": 0.22},\n",
        "        inter_prob=0.6\n",
        "    )\n",
        "]\n",
        "\n",
        "user_data = pd.read_excel('user_data.xlsx')\n",
        "music_data_loaded = pd.read_json('music_data.json', orient='records', lines=True)\n",
        "music_data = music_data_loaded.to_dict('records')\n",
        "\n",
        "gender_counts = user_data['Gender'].value_counts(normalize=True)\n",
        "gender_options = gender_counts.index.tolist()\n",
        "gender_probabilities = gender_counts.values.tolist()\n",
        "\n",
        "age_counts = user_data['Age'].value_counts(normalize=True)\n",
        "age_options = age_counts.index.tolist()\n",
        "age_probabilities = age_counts.values.tolist()\n",
        "\n",
        "subscription_plan_counts = user_data['spotify_subscription_plan'].value_counts(normalize=True)\n",
        "subscription_plan_options = subscription_plan_counts.index.tolist()\n",
        "subscription_plan_probabilities = subscription_plan_counts.values.tolist()\n",
        "\n",
        "listening_device_counts = user_data['spotify_listening_device'].value_counts(normalize=True)\n",
        "listening_device_options = listening_device_counts.index.tolist()\n",
        "listening_device_probabilities = listening_device_counts.values.tolist()\n",
        "\n",
        "schema = {\n",
        "  \"type\": \"record\",\n",
        "  \"name\": \"SpotifyWrappedData\",\n",
        "  \"namespace\": \"com.spotify.wrapped\",\n",
        "  \"fields\": [\n",
        "    {\"name\": \"UserId\", \"type\": \"string\"},\n",
        "    {\"name\": \"Age\", \"type\": \"string\"},\n",
        "    {\"name\": \"Gender\", \"type\": \"string\"},\n",
        "    {\"name\": \"ListeningDevice\", \"type\": \"string\"},\n",
        "    {\"name\": \"SubscriptionPlan\", \"type\": \"string\"},\n",
        "    {\"name\": \"MusicTimeSlot\", \"type\": \"string\"},\n",
        "    {\"name\": \"Location\", \"type\": \"string\"},\n",
        "    {\"name\": \"SongId\", \"type\": \"string\"},\n",
        "    {\"name\": \"TrackName\", \"type\": \"string\"},\n",
        "    {\"name\": \"Artist\", \"type\": \"string\"},\n",
        "    {\"name\": \"Genre\", \"type\": \"string\"},\n",
        "    {\"name\": \"SongStart\", \"type\": \"string\"},\n",
        "    {\"name\": \"SongEnd\", \"type\": \"string\"},\n",
        "    {\"name\": \"Length\", \"type\": \"int\"},\n",
        "    {\"name\": \"InteractionType\", \"type\": { \"type\": \"enum\", \"name\": \"InteractionTypeEnum\",\n",
        "    \"symbols\": [\"PLAY\", \"PAUSE\", \"SKIP\", \"LIKE\", \"ADDED_TO_PLAYLIST\"]}},\n",
        "    {\"name\": \"InteractionTimestamp\", \"type\": \"string\"}\n",
        "  ]\n",
        "}\n",
        "\n",
        "parsed_schema = fastavro.parse_schema(schema)\n",
        "\n",
        "\n",
        "def generate_listening_session(user_id, user_device, user_country, track_duration, interaction_type, session_start, track_end):\n",
        "    record = {\n",
        "          \"UserId\": user_id,\n",
        "          \"Age\": str(np.random.choice(age_options, p=age_probabilities)),\n",
        "          \"Gender\": np.random.choice(gender_options, p=gender_probabilities),\n",
        "          \"ListeningDevice\": user_device,\n",
        "          \"SubscriptionPlan\": np.random.choice(subscription_plan_options, p=subscription_plan_probabilities),\n",
        "          \"MusicTimeSlot\": \"Morning\" if 5 <= session_start.hour < 12 else \"Afternoon\" if 12 <= session_start.hour < 18 else \"Evening\",\n",
        "          \"Location\": user_country,\n",
        "          \"SongId\": song['track_id'],\n",
        "          \"TrackName\": song['track_name'],\n",
        "          \"Artist\": song['artists'],\n",
        "          \"Genre\": song['track_genre'],\n",
        "          \"Length\": track_duration,\n",
        "          \"SongStart\": session_start.strftime('%Y-%m-%dT%H:%M:%S%z'),\n",
        "          \"SongEnd\": track_end.strftime('%Y-%m-%dT%H:%M:%S%z'),\n",
        "          \"InteractionType\": interaction_type,\n",
        "          \"InteractionTimestamp\": session_start.strftime('%Y-%m-%dT%H:%M:%S%z')\n",
        "      }\n",
        "\n",
        "    return record\n",
        "\n",
        "def serialize(message):\n",
        "    print(\"Serialize:\" + str(message))\n",
        "    schemaless_bytes_writer = io.BytesIO()\n",
        "    fastavro.schemaless_writer(schemaless_bytes_writer, schema, message)\n",
        "    return schemaless_bytes_writer.getvalue()\n",
        "\n",
        "producer = KafkaProducer(\n",
        "    bootstrap_servers=['127.0.0.1:9092'],\n",
        "    value_serializer=serialize\n",
        ")\n",
        "\n",
        "topic_name_default=\"spotifyWrapped\"\n",
        "if len(sys.argv) > 1:\n",
        "    topic_name = sys.argv[1]\n",
        "else:\n",
        "    topic_name = topic_name_default\n",
        "\n",
        "start_time = datetime.strptime('2024-04-01T00:00:00-05:00', '%Y-%m-%dT%H:%M:%S%z')\n",
        "end_time = datetime.strptime('2024-04-01T23:59:59-05:00', '%Y-%m-%dT%H:%M:%S%z')\n",
        "\n",
        "for i in range(10000):\n",
        "    user_country = fake.country()\n",
        "    user_device = np.random.choice(listening_device_options, p=listening_device_probabilities)\n",
        "\n",
        "    session_start = datetime.now()\n",
        "    user_id = str(uuid.uuid4())\n",
        "    personality = random.choice(personalities)\n",
        "\n",
        "    genre = random.choice(list(personality.genre_distribution.keys()))\n",
        "\n",
        "    for j in range(round(50*personality.inter_prob)):\n",
        "\n",
        "        filtered_songs = [song for song in music_data if song['track_genre'] == genre]\n",
        "        if filtered_songs:\n",
        "            song = random.choice(filtered_songs)\n",
        "        else:\n",
        "            print(f\"No songs found for genre {genre}, selecting a random song instead.\")\n",
        "            song = random.choice(music_data)\n",
        "\n",
        "        # Decide interaction type based on personality\n",
        "        interaction_probabilities = [personality.interactions['skip_probability'],\n",
        "                                        personality.interactions['like_probability'],\n",
        "                                        personality.interactions.get('playlist_add_probability', 0)]\n",
        "        interaction_types = ['SKIP', 'LIKE', 'ADDED_TO_PLAYLIST']\n",
        "        interaction_type = random.choices(interaction_types + ['PLAY'], weights=interaction_probabilities + [1-sum(interaction_probabilities)], k=1)[0]\n",
        "\n",
        "        track_duration = song['duration_ms']\n",
        "        if interaction_type == 'SKIP':\n",
        "            track_duration = track_duration*random.random()\n",
        "\n",
        "        track_end = (start_time + timedelta(seconds=track_duration))\n",
        "\n",
        "        message = generate_listening_session(user_id, user_device, user_country, track_duration, interaction_type, session_start, track_end)\n",
        "        start_time = end_time\n",
        "\n",
        "        print(message)\n",
        "        producer.send(topic_name, value=message)\n",
        "        time.sleep(1)\n",
        "\n",
        "# Flush the producer\n",
        "producer.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrDMo1hLgIAl",
        "outputId": "45e381ad-2405-4b5c-ef52-309a8faa9a86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: redirecting stderr to stdout\n"
          ]
        }
      ],
      "source": [
        "!nohup python avro_producer.py > avro_producer.log &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CZkqw6rTgIAl"
      },
      "outputs": [],
      "source": [
        "!sleep 5\n",
        "!tail -20 avro_producer.log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqUFs8IvNOwp"
      },
      "source": [
        "## AVRO CONSUMER FILE GENERATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMlfDKqfgIAl",
        "outputId": "ae506e4a-99a8-4516-c6b2-fce3dc50b9c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing avro_consumer.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile avro_consumer.py\n",
        "\n",
        "from kafka import KafkaConsumer\n",
        "import fastavro\n",
        "from fastavro import parse_schema\n",
        "import io\n",
        "import time\n",
        "import sys\n",
        "\n",
        "# AVRO schema definition to the consumer\n",
        "schema = {\n",
        "  \"type\": \"record\",\n",
        "  \"name\": \"SpotifyWrappedData\",\n",
        "  \"namespace\": \"com.spotify.wrapped\",\n",
        "  \"fields\": [\n",
        "    {\"name\": \"UserId\", \"type\": \"string\"},\n",
        "    {\"name\": \"Age\", \"type\": \"string\"},\n",
        "    {\"name\": \"Gender\", \"type\": \"string\"},\n",
        "    {\"name\": \"ListeningDevice\", \"type\": \"string\"},\n",
        "    {\"name\": \"SubscriptionPlan\", \"type\": \"string\"},\n",
        "    {\"name\": \"MusicTimeSlot\", \"type\": \"string\"},\n",
        "    {\"name\": \"Location\", \"type\": \"string\"},\n",
        "    {\"name\": \"SongId\", \"type\": \"string\"},\n",
        "    {\"name\": \"TrackName\", \"type\": \"string\"},\n",
        "    {\"name\": \"Artist\", \"type\": \"string\"},\n",
        "    {\"name\": \"Genre\", \"type\": \"string\"},\n",
        "    {\"name\": \"SongStart\", \"type\": \"string\"},\n",
        "    {\"name\": \"SongEnd\", \"type\": \"string\"},\n",
        "    {\"name\": \"Length\", \"type\": \"int\"},\n",
        "    {\"name\": \"InteractionType\",\"type\": { \"type\": \"enum\", \"name\": \"InteractionTypeEnum\",\n",
        "        \"symbols\": [\"PLAY\", \"PAUSE\", \"SKIP\", \"LIKE\", \"ADDED_TO_PLAYLIST\"]}},\n",
        "    {\"name\": \"InteractionTimestamp\", \"type\": \"string\"}\n",
        "  ]\n",
        "}\n",
        "\n",
        "parsed_schema = parse_schema(schema)\n",
        "\n",
        "def deserialize(message):\n",
        "  # print(\"Deserialize:\" + str(message))\n",
        "  schemaless_bytes_reader = io.BytesIO(message)\n",
        "  try:\n",
        "    record=fastavro.schemaless_reader(schemaless_bytes_reader, schema)\n",
        "    return record\n",
        "  except Exception as ex:\n",
        "    print(ex)\n",
        "    return \"pass\"\n",
        "\n",
        "topic_name_default=\"spotifyWrapped\"\n",
        "if len(sys.argv) > 1:\n",
        "  topic_name = sys.argv[1]\n",
        "else:\n",
        "  topic_name = topic_name_default\n",
        "\n",
        "consumer = KafkaConsumer(\n",
        "    topic_name,\n",
        "    bootstrap_servers=['localhost:9092'],\n",
        "    auto_offset_reset='earliest',\n",
        "    enable_auto_commit=True,\n",
        "    group_id='AVRO_Consumer',\n",
        "    value_deserializer=deserialize #lambda v: fastavro.schemaless_reader(io.BytesIO(v), schema)\n",
        ")\n",
        "\n",
        "# Consume messages from the topic and print them\n",
        "for message in consumer:\n",
        "    print(\"=\"*10)\n",
        "    print(message.value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5eZl4LNgIAm",
        "outputId": "15a772b8-427a-4c9d-f862-dc933d6c2a11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: redirecting stderr to stdout\n"
          ]
        }
      ],
      "source": [
        "!nohup python avro_consumer.py > avro_consumer.log &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TIwGoiESgIAm"
      },
      "outputs": [],
      "source": [
        "!sleep 5\n",
        "!tail -20 avro_consumer.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw7Bf0KRgIAm",
        "outputId": "5e287312-6077-4ad9-8923-25aeff9712fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        4866       1 34 19:30 ?        00:00:03 python3 avro_producer.py\n",
            "root        5206       1  4 19:30 ?        00:00:00 python3 avro_consumer.py\n",
            "root        5235     213  0 19:30 ?        00:00:00 /bin/bash -c ps -ef |grep avro\n",
            "root        5237    5235  0 19:30 ?        00:00:00 grep avro\n"
          ]
        }
      ],
      "source": [
        "!ps -ef |grep avro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2WbNvpugIAm",
        "outputId": "009d1ac7-fdfd-4e8a-ca9d-6ea20bfa70fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVRO_Consumer\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "%%shell\n",
        "source ./environment.sh\n",
        "kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLcupjF8gIAm",
        "outputId": "366d4b62-8b1c-4315-c51b-7e30eac56c29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                             HOST            CLIENT-ID\n",
            "AVRO_Consumer   spotifyWrapped  0          6               7               1               kafka-python-2.0.2-afe0164a-7b3c-4d1a-94ae-b10a2d50271c /127.0.0.1      kafka-python-2.0.2\n",
            "AVRO_Consumer   spotifyWrapped  1          0               0               0               kafka-python-2.0.2-afe0164a-7b3c-4d1a-94ae-b10a2d50271c /127.0.0.1      kafka-python-2.0.2\n",
            "AVRO_Consumer   spotifyWrapped  2          2               4               2               kafka-python-2.0.2-afe0164a-7b3c-4d1a-94ae-b10a2d50271c /127.0.0.1      kafka-python-2.0.2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "source ./environment.sh\n",
        "kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group AVRO_Consumer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1pfGx02gIAn",
        "outputId": "111b62bc-8676-4183-b5b4-6abd864e44a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                             HOST            CLIENT-ID\n",
            "AVRO_Consumer   spotifyWrapped  0          7               7               0               kafka-python-2.0.2-afe0164a-7b3c-4d1a-94ae-b10a2d50271c /127.0.0.1      kafka-python-2.0.2\n",
            "AVRO_Consumer   spotifyWrapped  1          0               2               2               kafka-python-2.0.2-afe0164a-7b3c-4d1a-94ae-b10a2d50271c /127.0.0.1      kafka-python-2.0.2\n",
            "AVRO_Consumer   spotifyWrapped  2          5               8               3               kafka-python-2.0.2-afe0164a-7b3c-4d1a-94ae-b10a2d50271c /127.0.0.1      kafka-python-2.0.2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "source ./environment.sh\n",
        "kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --all-groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8C1fMsFNTGg"
      },
      "source": [
        "## SPARK CONNECTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kBaLCGKMgIAn"
      },
      "outputs": [],
      "source": [
        "spark_release='spark-3.5.1'\n",
        "hadoop_version='hadoop3'\n",
        "import time\n",
        "import os\n",
        "\n",
        "start=time.time()\n",
        "os.environ['SPARK_RELEASE']=spark_release\n",
        "os.environ['HADOOP_VERSION']=hadoop_version\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_release}-bin-{hadoop_version}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mr5MypatgIAn",
        "outputId": "1a4b70d8-0b27-4f8e-a7f0-d567b1dc3cb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.1\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # install Java8\n",
        "!wget -q http://apache.osuosl.org/spark/${SPARK_RELEASE}/${SPARK_RELEASE}-bin-${HADOOP_VERSION}.tgz # download spark-3.3.X\n",
        "!tar xf ${SPARK_RELEASE}-bin-${HADOOP_VERSION}.tgz # unzip it\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "\n",
        "# install findspark\n",
        "# findspark find your Spark Distribution and sets necessary environment variables\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "# Check the pyspark version\n",
        "import pyspark\n",
        "print(pyspark.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_vx1YQV8gIAn"
      },
      "outputs": [],
      "source": [
        "kafka_brokers=\"127.0.0.1:9092\" # Can be a comma-separated list of brokers\n",
        "topic_name=\"spotifyWrapped\"\n",
        "\n",
        "schema = \"\"\"\n",
        "{\n",
        "  \"type\": \"record\",\n",
        "  \"name\": \"SpotifyWrappedData\",\n",
        "  \"namespace\": \"com.spotify.wrapped\",\n",
        "  \"fields\": [\n",
        "    {\"name\": \"UserId\", \"type\": \"string\"},\n",
        "    {\"name\": \"Age\", \"type\": \"string\"},\n",
        "    {\"name\": \"Gender\", \"type\": \"string\"},\n",
        "    {\"name\": \"ListeningDevice\", \"type\": \"string\"},\n",
        "    {\"name\": \"SubscriptionPlan\", \"type\": \"string\"},\n",
        "    {\"name\": \"MusicTimeSlot\", \"type\": \"string\"},\n",
        "    {\"name\": \"Location\", \"type\": \"string\"},\n",
        "    {\"name\": \"SongId\", \"type\": \"string\"},\n",
        "    {\"name\": \"TrackName\", \"type\": \"string\"},\n",
        "    {\"name\": \"Artist\", \"type\": \"string\"},\n",
        "    {\"name\": \"Genre\", \"type\": \"string\"},\n",
        "    {\"name\": \"SongStart\", \"type\": \"string\"},\n",
        "    {\"name\": \"SongEnd\", \"type\": \"string\"},\n",
        "    {\"name\": \"Length\", \"type\": \"int\"},\n",
        "    {\"name\": \"InteractionType\", \"type\": { \"type\": \"enum\", \"name\": \"InteractionTypeEnum\",\n",
        "        \"symbols\": [\"PLAY\", \"PAUSE\", \"SKIP\", \"LIKE\", \"ADDED_TO_PLAYLIST\"]}},\n",
        "    {\"name\": \"InteractionTimestamp\", \"type\": \"string\"}\n",
        "  ]\n",
        "}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "V2c-3sH5gIAn"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.avro.functions import from_avro\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"AVRO-Kafka_Streaming\") \\\n",
        "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-avro_2.12:3.5.0') \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "STvyhZu6gIAn"
      },
      "outputs": [],
      "source": [
        "# Kafka Configuration for reading from Kafka/Event Hub\n",
        "# Kafka source will create a unique group id for each query automatically. The user can set the prefix of the automatically\n",
        "# generated group.id’s via the optional source option groupIdPrefix, default value is “spark-kafka-source”.\n",
        "kafkaConf = {\n",
        "    \"kafka.bootstrap.servers\": kafka_brokers,\n",
        "    # Below settins required if kafka is secured:\n",
        "    # \"kafka.sasl.mechanism\": \"PLAIN\",\n",
        "    # \"kafka.security.protocol\": \"SASL_SSL\",\n",
        "    # \"kafka.sasl.jaas.config\": 'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"Endpoint=sb://eventhubname.servicebus.windows.net/;SharedAccessKeyName=listenpolicyforspark;SharedAccessKey=ckNkSjcyXKGN8FCIRIS3qtkKvW+AEhB6QPaM=;EntityPath=instructortest\";',\n",
        "    \"subscribe\": topic_name, # to read from specific partitions use option: \"assign\": {topic_name:[0,1]})\n",
        "    \"startingOffsets\": \"latest\", # \"earliest\", \"latest\"\n",
        "    \"enable.auto.commit\": \"true \",\n",
        "    \"groupIdPrefix\": \"Stream_Analytics_\",\n",
        "    \"auto.commit.interval.ms\": \"5000\"\n",
        "}\n",
        "\n",
        "\n",
        "# Read from Event Hub using Kafka\n",
        "df = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .options(**kafkaConf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dqh753oVgIAo",
        "outputId": "ce5a8a3e-47be-477e-9af6-88cf32339470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- key: binary (nullable = true)\n",
            " |-- value: binary (nullable = true)\n",
            " |-- topic: string (nullable = true)\n",
            " |-- partition: integer (nullable = true)\n",
            " |-- offset: long (nullable = true)\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- timestampType: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = df.load()\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiCKJzI8gIAo",
        "outputId": "de3d45f4-6f15-480a-9770-f2b216e6d094"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Column<'from_avro(value) AS SpotifyWrappedData'>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "from_avro(df.value, schema).alias(\"SpotifyWrappedData\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTfK_3ZtgIAo",
        "outputId": "508f462e-9a50-4c87-e1a1-f0eec6569ce5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- SpotifyWrappedData: struct (nullable = true)\n",
            " |    |-- UserId: string (nullable = false)\n",
            " |    |-- Age: string (nullable = false)\n",
            " |    |-- Gender: string (nullable = false)\n",
            " |    |-- ListeningDevice: string (nullable = false)\n",
            " |    |-- SubscriptionPlan: string (nullable = false)\n",
            " |    |-- MusicTimeSlot: string (nullable = false)\n",
            " |    |-- Location: string (nullable = false)\n",
            " |    |-- SongId: string (nullable = false)\n",
            " |    |-- TrackName: string (nullable = false)\n",
            " |    |-- Artist: string (nullable = false)\n",
            " |    |-- Genre: string (nullable = false)\n",
            " |    |-- SongStart: string (nullable = false)\n",
            " |    |-- SongEnd: string (nullable = false)\n",
            " |    |-- Length: integer (nullable = false)\n",
            " |    |-- InteractionType: string (nullable = false)\n",
            " |    |-- InteractionTimestamp: string (nullable = false)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = df.select(from_avro(df.value, schema).alias(\"SpotifyWrappedData\"))\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcAe7taogIAp",
        "outputId": "b15b1bf1-b99f-403f-a27c-36b2c8cf0ba7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- UserId: string (nullable = true)\n",
            " |-- Age: string (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- ListeningDevice: string (nullable = true)\n",
            " |-- SubscriptionPlan: string (nullable = true)\n",
            " |-- MusicTimeSlot: string (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- SongId: string (nullable = true)\n",
            " |-- TrackName: string (nullable = true)\n",
            " |-- Artist: string (nullable = true)\n",
            " |-- Genre: string (nullable = true)\n",
            " |-- Length: integer (nullable = true)\n",
            " |-- SongEnd: string (nullable = true)\n",
            " |-- SongStart: string (nullable = true)\n",
            " |-- InteractionType: string (nullable = true)\n",
            " |-- InteractionTimestamp: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Assuming 'df' is your DataFrame loaded with data conforming to the flattened Avro schema\n",
        "\n",
        "df = df.select(\n",
        "    col(\"SpotifyWrappedData.UserId\"),\n",
        "    col(\"SpotifyWrappedData.Age\"),\n",
        "    col(\"SpotifyWrappedData.Gender\"),\n",
        "    col(\"SpotifyWrappedData.ListeningDevice\"),\n",
        "    col(\"SpotifyWrappedData.SubscriptionPlan\"),\n",
        "    col(\"SpotifyWrappedData.MusicTimeSlot\"),\n",
        "    col(\"SpotifyWrappedData.Location\"),\n",
        "    col(\"SpotifyWrappedData.SongId\"),\n",
        "    col(\"SpotifyWrappedData.TrackName\"),\n",
        "    col(\"SpotifyWrappedData.Artist\"),\n",
        "    col(\"SpotifyWrappedData.Genre\"),\n",
        "    col(\"SpotifyWrappedData.Length\"),\n",
        "    col(\"SpotifyWrappedData.SongEnd\"),\n",
        "    col(\"SpotifyWrappedData.SongStart\"),\n",
        "    col(\"SpotifyWrappedData.InteractionType\"),\n",
        "    col(\"SpotifyWrappedData.InteractionTimestamp\")\n",
        ")\n",
        "\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L66NEvYXx-Hl"
      },
      "source": [
        "# Analyses Implementations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnF1mZJhLfom"
      },
      "source": [
        "### Query # 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, sum as sql_sum, when\n",
        "\n",
        "artist_popularity_query = df \\\n",
        "    .groupBy(col(\"Artist\")) \\\n",
        "    .agg(\n",
        "        sql_sum(when(col(\"InteractionType\") == \"PLAY\", 1).otherwise(0)).alias(\"Total_Plays\"),\n",
        "        sql_sum(when(col(\"InteractionType\") == \"LIKE\", 1).otherwise(0)).alias(\"Total_Likes\")\n",
        "    ) \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"artist_popularity\") \\\n",
        "    .start()\n",
        "\n",
        "time.sleep(30)\n",
        "\n",
        "print(\"Artist Popularity and Engagement Metrics:\")\n",
        "spark.sql(\"SELECT * FROM artist_popularity ORDER BY Total_Plays DESC, Total_Likes DESC\").show(truncate=False)\n",
        "\n",
        "artist_popularity_query.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0HMURkUAc3g",
        "outputId": "7dcd2e9e-981e-4e13-d075-2cef60782a34"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artist Popularity and Engagement Metrics:\n",
            "+------------------------------------------------------------------------------------------+-----------+-----------+\n",
            "|Artist                                                                                    |Total_Plays|Total_Likes|\n",
            "+------------------------------------------------------------------------------------------+-----------+-----------+\n",
            "|Wolfgang Amadeus Mozart;Ingrid Haebler;London Symphony Orchestra;Sir Colin Davis          |1          |0          |\n",
            "|Salim–Sulaiman;Shreya Ghoshal                                                             |1          |0          |\n",
            "|T. R. Mahalingam                                                                          |1          |0          |\n",
            "|Yiruma                                                                                    |0          |1          |\n",
            "|Wolfgang Amadeus Mozart;Danielle Laval                                                    |0          |1          |\n",
            "|Wolfgang Amadeus Mozart;Heinz Holliger;Academy of St. Martin in the Fields;Kenneth Sillito|0          |1          |\n",
            "|Achint                                                                                    |0          |1          |\n",
            "|M. S. Subbulakshmi                                                                        |0          |1          |\n",
            "|Antônio Carlos Jobim;Frank Sinatra                                                        |0          |0          |\n",
            "|Wolfgang Amadeus Mozart;Wiener Philharmoniker;Ferenc Fricsay                              |0          |0          |\n",
            "|Academy of St. Martin in the Fields;Sir Neville Marriner                                  |0          |0          |\n",
            "|Robert Schumann;Pavel Nersessian                                                          |0          |0          |\n",
            "|The Velvet Underground                                                                    |0          |0          |\n",
            "|Hariprasad Chaurasia                                                                      |0          |0          |\n",
            "+------------------------------------------------------------------------------------------+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Query 2"
      ],
      "metadata": {
        "id": "MIfVjPt-Adsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, count\n",
        "\n",
        "artist_region_popularity_query = df \\\n",
        "    .groupBy(\"Location\", \"Artist\") \\\n",
        "    .agg(\n",
        "        count(\"UserId\").alias(\"Listener_Count\")\n",
        "    ) \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"artist_region_popularity\") \\\n",
        "    .start()\n",
        "\n",
        "time.sleep(30)\n",
        "print(\"Artist Popularity Across Regions:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT Location, Artist, Listener_Count\n",
        "    FROM artist_region_popularity\n",
        "    ORDER BY Location, Listener_Count DESC\n",
        "\"\"\").show(truncate=False)\n",
        "\n",
        "artist_region_popularity_query.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dG1ga_C6AeCK",
        "outputId": "1e546b77-eed3-4965-d2f1-639a9b91c931"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artist Popularity Across Regions:\n",
            "+--------+--------------------------------------------------------------------------------+--------------+\n",
            "|Location|Artist                                                                          |Listener_Count|\n",
            "+--------+--------------------------------------------------------------------------------+--------------+\n",
            "|China   |Wolfgang Amadeus Mozart;Danielle Laval                                          |2             |\n",
            "|China   |Ottorino Respighi;Berliner Philharmoniker;Herbert von Karajan                   |1             |\n",
            "|China   |Chicago Symphony Orchestra;Claudio Abbado                                       |1             |\n",
            "|China   |Sirkazhi Govindarajan                                                           |1             |\n",
            "|China   |Johannes Brahms;Music Lab Collective                                            |1             |\n",
            "|China   |Begum Akhtar                                                                    |1             |\n",
            "|China   |Chilly Gonzales;Johannes Bornlöf                                                |1             |\n",
            "|China   |Wolfgang Amadeus Mozart;Gürzenich Orchestra Köln;Günter Wand                    |1             |\n",
            "|China   |Wolfgang Amadeus Mozart;Academy of St. Martin in the Fields;Sir Neville Marriner|1             |\n",
            "|China   |Ludwig van Beethoven;Nelly Kokinos                                              |1             |\n",
            "|China   |Robert Schumann;Pavel Nersessian                                                |1             |\n",
            "|China   |V. Selvaganesh;Karthik;Chinmayi                                                 |1             |\n",
            "|China   |Pyotr Ilyich Tchaikovsky;phil Blech Wien;Anton Mittermayr                       |1             |\n",
            "|China   |M. Balamuralikrishna                                                            |1             |\n",
            "|China   |Parveen Sultana                                                                 |1             |\n",
            "|China   |Antonio Vivaldi;Renato Fasano;Luigi Ferro;I virtuosi di Roma                    |1             |\n",
            "|China   |Bhimsen Joshi                                                                   |1             |\n",
            "|China   |Gabriel Fauré;Kun-Woo Paik                                                      |1             |\n",
            "|China   |Wolfgang Amadeus Mozart;Arthur Grumiaux;Walter Klien                            |1             |\n",
            "|China   |Carlos Cipa                                                                     |1             |\n",
            "+--------+--------------------------------------------------------------------------------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 3"
      ],
      "metadata": {
        "id": "n_AVJ_5XAoIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, sum as sql_sum\n",
        "\n",
        "engagement_query = df \\\n",
        "    .groupBy(col(\"UserId\")) \\\n",
        "    .agg(\n",
        "        sql_sum(when(col(\"InteractionType\") == \"PLAY\", 1).otherwise(0)).alias(\"Plays\"),\n",
        "        sql_sum(when(col(\"InteractionType\") == \"SKIP\", 1).otherwise(0)).alias(\"Skips\"),\n",
        "        sql_sum(when(col(\"InteractionType\") == \"LIKE\", 1).otherwise(0)).alias(\"Likes\")\n",
        "    ) \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"user_engagement\") \\\n",
        "    .start()\n",
        "\n",
        "time.sleep(30)\n",
        "\n",
        "print(\"User Engagement:\")\n",
        "spark.sql(\"SELECT * FROM user_engagement ORDER BY Plays DESC, Likes DESC\").show(truncate=False)\n",
        "\n",
        "engagement_query.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3cPGF5pAobw",
        "outputId": "53372bce-b4e7-43ae-e66a-02393a881bb6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User Engagement:\n",
            "+------------------------------------+-----+-----+-----+\n",
            "|UserId                              |Plays|Skips|Likes|\n",
            "+------------------------------------+-----+-----+-----+\n",
            "|3624147e-c71a-40c3-b048-ef8baef774fc|5    |4    |6    |\n",
            "|642e89a4-3899-4c9c-9c3d-e7806f264a28|1    |1    |2    |\n",
            "+------------------------------------+-----+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1zLfSUCb0Mf"
      },
      "source": [
        "## Other Considered Queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouWRlZHR04T9",
        "outputId": "1e1aefaf-5088-431e-f3ed-2f2de8a7bbe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listening Duration by Genre and Location per Window:\n",
            "+------+-----+--------+----------------------+\n",
            "|window|Genre|Location|TotalListeningDuration|\n",
            "+------+-----+--------+----------------------+\n",
            "+------+-----+--------+----------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, window, to_timestamp, sum\n",
        "\n",
        "df = df.withColumn(\"SongStart\", to_timestamp(col(\"SongStart\"), \"yyyy-MM-dd'T'HH:mm:ssZ\"))\n",
        "df = df.withColumn(\"SongEnd\", to_timestamp(col(\"SongEnd\"), \"yyyy-MM-dd'T'HH:mm:ssZ\"))\n",
        "\n",
        "df = df.withColumn(\"ListeningDuration\", (col(\"SongEnd\").cast(\"long\") - col(\"SongStart\").cast(\"long\")))\n",
        "\n",
        "\n",
        "windowDuration = \"5 minutes\"\n",
        "slideDuration = \"1 minute\"\n",
        "\n",
        "genre_location_duration_counts = df.groupBy(\n",
        "    window(col(\"SongStart\"), windowDuration, slideDuration),\n",
        "    col(\"Genre\"),\n",
        "    col(\"Location\")\n",
        ").agg(\n",
        "    sum(\"ListeningDuration\").alias(\"TotalListeningDuration\")\n",
        ")\n",
        "\n",
        "query_name = \"genre_popularity_by_location_query\"\n",
        "genre_location_query = genre_location_duration_counts.writeStream \\\n",
        "    .queryName(query_name) \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .start()\n",
        "\n",
        "import time\n",
        "time.sleep(30)\n",
        "\n",
        "print(\"Listening Duration by Genre and Location per Window:\")\n",
        "spark.sql(f\"SELECT * FROM {query_name} ORDER BY window DESC, TotalListeningDuration DESC\").show(truncate=False)\n",
        "\n",
        "genre_location_query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, window, to_timestamp, when, count\n",
        "\n",
        "df = df.withColumn(\"SongStart\", to_timestamp(col(\"SongStart\"), \"yyyy-MM-dd'T'HH:mm:ssZ\"))\n",
        "df = df.withColumn(\"SongEnd\", to_timestamp(col(\"SongEnd\"), \"yyyy-MM-dd'T'HH:mm:ssZ\"))\n",
        "\n",
        "df_likes = df.filter(col(\"InteractionType\") == \"LIKE\")\n",
        "\n",
        "df_likes = df_likes.withWatermark(\"SongStart\", \"1 hour\")\n",
        "\n",
        "windowDuration = \"5 minutes\"\n",
        "slideDuration = \"1 minute\"\n",
        "\n",
        "song_artist_location_likes_counts = df_likes.groupBy(\n",
        "    window(col(\"SongStart\"), windowDuration, slideDuration),\n",
        "    col(\"TrackName\"),\n",
        "    col(\"Artist\"),\n",
        "    col(\"Location\")\n",
        ").count().alias(\"LikesCount\")\n",
        "\n",
        "query_name = \"song_popularity_by_artist_and_location_query\"\n",
        "song_artist_location_query = song_artist_location_likes_counts.writeStream \\\n",
        "    .queryName(query_name) \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .start()\n",
        "\n",
        "import time\n",
        "time.sleep(30)\n",
        "\n",
        "print(\"Likes Count by Song, Artist, and Location per Window:\")\n",
        "spark.sql(f\"SELECT * FROM {query_name} ORDER BY window DESC, count DESC\").show(truncate=False)\n",
        "\n",
        "song_artist_location_query.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aErTt3RiiQKz",
        "outputId": "749772e6-6755-4ad7-86b1-8ce3535e2128"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Likes Count by Song, Artist, and Location per Window:\n",
            "+------+---------+------+--------+-----+\n",
            "|window|TrackName|Artist|Location|count|\n",
            "+------+---------+------+--------+-----+\n",
            "+------+---------+------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGYM8NsXnebj",
        "outputId": "e58f8f5c-aade-4175-ba1e-5dbc5a1f72b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Demographics Analysis:\n",
            "+-----+------+-----+\n",
            "|Age  |Gender|count|\n",
            "+-----+------+-----+\n",
            "|20-35|Female|18   |\n",
            "|12-20|Female|4    |\n",
            "|20-35|Male  |3    |\n",
            "|20-35|Others|1    |\n",
            "|12-20|Male  |1    |\n",
            "+-----+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from pyspark.sql.functions import col, window\n",
        "\n",
        "demographics_query = df.groupBy(col(\"Age\"), col(\"Gender\")).count() \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"demographics\") \\\n",
        "    .start()\n",
        "\n",
        "time.sleep(30)\n",
        "print(\"Demographics Analysis:\")\n",
        "spark.sql(\"SELECT * FROM demographics ORDER BY count DESC\").show(truncate=False)\n",
        "demographics_query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVUQpprtlFIQ",
        "outputId": "973308fb-26de-4d44-a763-c2223f268263"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listening Habits Analysis:\n",
            "+------------------------------------------+-------------+-----+\n",
            "|window                                    |MusicTimeSlot|count|\n",
            "+------------------------------------------+-------------+-----+\n",
            "|{2024-04-15 19:00:00, 2024-04-15 20:00:00}|Evening      |27   |\n",
            "+------------------------------------------+-------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import window, avg\n",
        "\n",
        "listening_habits_query = df.groupBy(window(col(\"InteractionTimestamp\"), \"1 hour\"), col(\"MusicTimeSlot\")).count() \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"listening_habits\") \\\n",
        "    .start()\n",
        "\n",
        "time.sleep(30)\n",
        "print(\"Listening Habits Analysis:\")\n",
        "spark.sql(\"SELECT * FROM listening_habits ORDER BY window\").show(truncate=False)\n",
        "listening_habits_query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mx7yZW69lO8J",
        "outputId": "09d298f1-ec3b-4163-cc2c-41970599a824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Genre Popularity Analysis:\n",
            "+----------+-----+------+-----+\n",
            "|Genre     |Age  |Gender|count|\n",
            "+----------+-----+------+-----+\n",
            "|electronic|20-35|Female|11   |\n",
            "|electronic|20-35|Male  |3    |\n",
            "|soul      |20-35|Male  |2    |\n",
            "|soul      |12-20|Female|2    |\n",
            "|electronic|35-60|Male  |2    |\n",
            "|soul      |35-60|Male  |1    |\n",
            "|electronic|20-35|Others|1    |\n",
            "|soul      |20-35|Female|1    |\n",
            "|electronic|12-20|Male  |1    |\n",
            "|electronic|12-20|Female|1    |\n",
            "+----------+-----+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "genre_popularity_query = df.groupBy(col(\"Genre\"), col(\"Age\"), col(\"Gender\")).count() \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"genre_popularity\") \\\n",
        "    .start()\n",
        "\n",
        "time.sleep(30)\n",
        "print(\"Genre Popularity Analysis:\")\n",
        "spark.sql(\"SELECT * FROM genre_popularity ORDER BY count DESC\").show(truncate=False)\n",
        "genre_popularity_query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ85kbagmTw2",
        "outputId": "3a196cf3-3ff3-41aa-ffe7-5f47a315a15c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Genre Location Analysis:\n",
            "+----------+---------------+-----+\n",
            "|Genre     |Location       |count|\n",
            "+----------+---------------+-----+\n",
            "|folk      |North Macedonia|20   |\n",
            "|pop       |Turkmenistan   |3    |\n",
            "|electronic|Puerto Rico    |2    |\n",
            "+----------+---------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "genre_location_query = df.groupBy(col(\"Genre\"), col(\"Location\")).count() \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"genre_location_popularity\") \\\n",
        "    .start()\n",
        "\n",
        "\n",
        "time.sleep(30)\n",
        "print(\"Genre Location Analysis:\")\n",
        "spark.sql(\"SELECT * FROM genre_location_popularity ORDER BY count DESC\").show(truncate=False)\n",
        "genre_location_query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w18nAHUFE9af",
        "outputId": "e90d4ebc-fe70-40d9-f9e5-2199ca1eb486"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+------------------------+\n",
            "|window|Genre|AverageListeningDuration|\n",
            "+------+-----+------------------------+\n",
            "+------+-----+------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import unix_timestamp, col, to_timestamp\n",
        "\n",
        "df = df.withColumn(\"SongStartTimestamp\", to_timestamp(col(\"SongStart\"), \"yyyy-MM-dd'T'HH:mm:ssZ\"))\n",
        "df = df.withColumn(\"SongEndTimestamp\", to_timestamp(col(\"SongEnd\"), \"yyyy-MM-dd'T'HH:mm:ssZ\"))\n",
        "\n",
        "\n",
        "df = df.withColumn(\"ListeningDuration\", (col(\"SongEnd\") - col(\"SongStart\")))\n",
        "\n",
        "from pyspark.sql.functions import window, avg\n",
        "\n",
        "windowDuration = \"5 minutes\"\n",
        "slideDuration = \"2 minutes\"\n",
        "\n",
        "listening_duration_query = df.groupBy(\n",
        "    window(col(\"SongStart\"), windowDuration, slideDuration), \"Genre\"\n",
        ").agg(\n",
        "    avg(\"ListeningDuration\").alias(\"AverageListeningDuration\")\n",
        ")\n",
        "\n",
        "query_name = \"listening_duration_query\"\n",
        "listening_time_query = listening_duration_query.writeStream \\\n",
        "    .queryName(query_name) \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .start()\n",
        "\n",
        "import time\n",
        "time.sleep(30)\n",
        "\n",
        "spark.sql(f\"SELECT * FROM {query_name} ORDER BY window, AverageListeningDuration ASC\").show(truncate=False)\n",
        "\n",
        "listening_time_query.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWdOco6zHK5g",
        "outputId": "7d673341-e4e2-462c-8ffa-9d758a5a4007"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----+\n",
            "|InteractionType  |count|\n",
            "+-----------------+-----+\n",
            "|PLAY             |8    |\n",
            "|LIKE             |11   |\n",
            "|ADDED_TO_PLAYLIST|8    |\n",
            "+-----------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "import time\n",
        "\n",
        "interaction_counts = df.groupBy(\"InteractionType\").count()\n",
        "\n",
        "interaction_query = interaction_counts \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"interactions\") \\\n",
        "    .start()\n",
        "\n",
        "time.sleep(30)\n",
        "\n",
        "spark.sql(\"SELECT * FROM interactions DESC\").show(truncate=False)\n",
        "#print(interaction_query.lastProgress)\n",
        "\n",
        "interaction_query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, window, to_timestamp, count\n",
        "\n",
        "df = df.withColumn(\"SongStart\", to_timestamp(col(\"SongStart\"), \"yyyy-MM-dd'T'HH:mm:ssZ\"))\n",
        "df_likes = df.filter(col(\"InteractionType\") == \"LIKE\")\n",
        "\n",
        "df_likes = df_likes.withWatermark(\"SongStart\", \"2 hours\")\n",
        "\n",
        "windowDuration = \"1 hour\"\n",
        "slideDuration = \"30 minutes\"\n",
        "\n",
        "song_artist_location_likes_counts = df_likes.groupBy(\n",
        "    window(col(\"SongStart\"), windowDuration, slideDuration),\n",
        "    col(\"TrackName\"),\n",
        "    col(\"Artist\"),\n",
        "    col(\"Location\")\n",
        ").count().alias(\"LikesCount\")\n",
        "\n",
        "query_name = \"extended_song_popularity_by_artist_and_location_query\"\n",
        "song_artist_location_query = song_artist_location_likes_counts.writeStream \\\n",
        "    .queryName(query_name) \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .start()\n",
        "\n",
        "import time\n",
        "time.sleep(30)\n",
        "\n",
        "print(\"Extended Likes Count by Song, Artist, and Location per Window:\")\n",
        "spark.sql(f\"SELECT * FROM {query_name} ORDER BY window DESC, count DESC\").show(truncate=False)\n",
        "\n",
        "song_artist_location_query.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LruHlheQoM6L",
        "outputId": "ac22f8d7-9b81-4305-a5be-62f49c774407"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extended Likes Count by Song, Artist, and Location per Window:\n",
            "+------+---------+------+--------+-----+\n",
            "|window|TrackName|Artist|Location|count|\n",
            "+------+---------+------+--------+-----+\n",
            "+------+---------+------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, window, to_timestamp, sum\n",
        "import time\n",
        "\n",
        "df = df.withColumn(\"SongStart\", to_timestamp(col(\"SongStart\"), \"yyyy-MM-dd'T'HH:mm:ssZ\"))\n",
        "df = df.withColumn(\"SongEnd\", to_timestamp(col(\"SongEnd\"), \"yyyy-MM-dd'T'HH:mm:ssZ\"))\n",
        "\n",
        "df = df.withColumn(\"ListeningDuration\", (col(\"SongEnd\").cast(\"long\") - col(\"SongStart\").cast(\"long\")))\n",
        "\n",
        "df = df.withWatermark(\"SongStart\", \"1 hour\")\n",
        "\n",
        "windowDuration = \"30 minutes\"\n",
        "slideDuration = \"15 minutes\"\n",
        "\n",
        "genre_duration_counts = df.groupBy(\n",
        "    window(col(\"SongStart\"), windowDuration, slideDuration),\n",
        "    col(\"Genre\")\n",
        ").agg(\n",
        "    sum(\"ListeningDuration\").alias(\"TotalListeningDuration\")\n",
        ")\n",
        "\n",
        "query_name = \"top_genres_by_listening_duration_query\"\n",
        "top_genres_query = genre_duration_counts.writeStream \\\n",
        "    .queryName(query_name) \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .start()\n",
        "\n",
        "time.sleep(30)\n",
        "\n",
        "print(\"Listening Duration by Genre per Window:\")\n",
        "spark.sql(f\"SELECT * FROM {query_name} ORDER BY window DESC, TotalListeningDuration DESC\").show(truncate=False)\n",
        "\n",
        "top_genres_query.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1nybEJytQ9K",
        "outputId": "1ab72a75-d5c1-4090-aa82-e688aa7f0bab"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listening Duration by Genre per Window:\n",
            "+------+-----+----------------------+\n",
            "|window|Genre|TotalListeningDuration|\n",
            "+------+-----+----------------------+\n",
            "+------+-----+----------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7HjJHYcWNcqu",
        "00O0ugghNGXu",
        "uHCcnPFwNJOV",
        "IqUFs8IvNOwp",
        "L8C1fMsFNTGg",
        "L66NEvYXx-Hl",
        "n_AVJ_5XAoIZ",
        "H1zLfSUCb0Mf"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}